{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 21:58:13.366266: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732553893.388182   14723 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732553893.395029   14723 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-25 21:58:13.416866: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os, joblib\n",
    "import pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding, Bidirectional, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import torch\n",
    "import torch.distributed as dist \n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# from google.colab import drive\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Embedding, LSTM, GRU, Bidirectional, Dense\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abbasinYousuf': 0, 'ajmalKhattak': 1, 'allamaAbdulHai': 2, 'ghaniKhan': 3, 'hamzaBaba': 4, 'javedAhmedzai': 5, 'karanKhan': 6, 'khatirAfridi': 7, 'khushalKhanKhattak': 8, 'matiullahTurab': 9, 'mumtazOrakazi': 10, 'munirJan': 11, 'naeemAhmed': 12, 'rabiaMumtaz': 13, 'rahmanBaba': 14, 'rehmatShah': 15, 'sahibShahSabir': 16, 'shabbirKhanDurrani': 17, 'shakirOrakzai': 18, 'shoaibKhanKhattak': 19}\n"
     ]
    }
   ],
   "source": [
    "# Define Data directory\n",
    "DATA_DIR = '../data/'\n",
    "\n",
    "# list of poet names\n",
    "# Removed the 5 lowest poets\n",
    "poets = [\"abbasinYousuf\", \"ajmalKhattak\", \"allamaAbdulHai\", \"ghaniKhan\", \"hamzaBaba\", \"javedAhmedzai\", \"karanKhan\", \"khatirAfridi\", \"khushalKhanKhattak\", \"matiullahTurab\", \"mumtazOrakazi\", \"munirJan\", \"naeemAhmed\", \"rabiaMumtaz\", \"rahmanBaba\", \"rehmatShah\", \"sahibShahSabir\", \"shabbirKhanDurrani\", \"shakirOrakzai\", \"shoaibKhanKhattak\"]\n",
    "\n",
    "poet_labels = {poet: i for i, poet in enumerate(poets)}\n",
    "print(poet_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Label the Data\n",
    "def load_and_label(data_dir, poets, poet_labels):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for poet in poets:\n",
    "        poet_dir = os.path.join(data_dir, poet)\n",
    "        file_path = os.path.join(poet_dir, f'{poet}.txt')\n",
    "\n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f'{file_path} does not exist')\n",
    "            continue\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            data.extend(lines)\n",
    "            labels.extend([poet_labels[poet]] * len(lines))\n",
    "\n",
    "    df = pd.DataFrame({'text': data, 'label': labels})\n",
    "    return data, labels, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>54620.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.625265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.352617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label\n",
       "count  54620.000000\n",
       "mean       8.625265\n",
       "std        5.352617\n",
       "min        0.000000\n",
       "25%        4.000000\n",
       "50%        8.000000\n",
       "75%       14.000000\n",
       "max       19.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, labels, df = load_and_label(DATA_DIR, poets, poet_labels)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems_train, poems_temp, lables_train, labels_temp = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "poems_val, poems_test, labels_val, labels_test = train_test_split(poems_temp, labels_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad81000e2eb947e799f0815d880fe4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/62.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085426ff85eb41f6a71efdbc2947a987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/516 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a75229431af487ca47b47ad946e9204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.61M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b052ea733b4184ae12176aefff7270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d014fa664e403a9c40b5b225dfbd1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfc984a07924a569210d33e499805bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/507M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at urduhack/roberta-urdu-small were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(52000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=52000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a178516401492fb2ce9f00491984c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/507M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"urduhack/roberta-urdu-small\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"urduhack/roberta-urdu-small\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM roBERTa Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save embeddings\n",
    "def save_embeddings(texts, prefix, model, tokenizer, device, max_length=80):\n",
    "    for i, text in enumerate(texts):\n",
    "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "            # Use the mean of the last hidden state as the embedding\n",
    "            embedding = outputs.hidden_states[-1].mean(dim=1).to(device)\n",
    "        \n",
    "        torch.save(embedding, f'./models_roberta_urdu/{prefix}_embedding_{i}.pt')\n",
    "        if i % 100 == 0:\n",
    "            print(f\"{prefix.capitalize()} Embedding {i} saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Embedding 0 saved.\n",
      "Train Embedding 100 saved.\n",
      "Train Embedding 200 saved.\n",
      "Train Embedding 300 saved.\n",
      "Train Embedding 400 saved.\n",
      "Train Embedding 500 saved.\n",
      "Train Embedding 600 saved.\n",
      "Train Embedding 700 saved.\n",
      "Train Embedding 800 saved.\n",
      "Train Embedding 900 saved.\n",
      "Train Embedding 1000 saved.\n",
      "Train Embedding 1100 saved.\n",
      "Train Embedding 1200 saved.\n",
      "Train Embedding 1300 saved.\n",
      "Train Embedding 1400 saved.\n",
      "Train Embedding 1500 saved.\n",
      "Train Embedding 1600 saved.\n",
      "Train Embedding 1700 saved.\n",
      "Train Embedding 1800 saved.\n",
      "Train Embedding 1900 saved.\n",
      "Train Embedding 2000 saved.\n",
      "Train Embedding 2100 saved.\n",
      "Train Embedding 2200 saved.\n",
      "Train Embedding 2300 saved.\n",
      "Train Embedding 2400 saved.\n",
      "Train Embedding 2500 saved.\n",
      "Train Embedding 2600 saved.\n",
      "Train Embedding 2700 saved.\n",
      "Train Embedding 2800 saved.\n",
      "Train Embedding 2900 saved.\n",
      "Train Embedding 3000 saved.\n",
      "Train Embedding 3100 saved.\n",
      "Train Embedding 3200 saved.\n",
      "Train Embedding 3300 saved.\n",
      "Train Embedding 3400 saved.\n",
      "Train Embedding 3500 saved.\n",
      "Train Embedding 3600 saved.\n",
      "Train Embedding 3700 saved.\n",
      "Train Embedding 3800 saved.\n",
      "Train Embedding 3900 saved.\n",
      "Train Embedding 4000 saved.\n",
      "Train Embedding 4100 saved.\n",
      "Train Embedding 4200 saved.\n",
      "Train Embedding 4300 saved.\n",
      "Train Embedding 4400 saved.\n",
      "Train Embedding 4500 saved.\n",
      "Train Embedding 4600 saved.\n",
      "Train Embedding 4700 saved.\n",
      "Train Embedding 4800 saved.\n",
      "Train Embedding 4900 saved.\n",
      "Train Embedding 5000 saved.\n",
      "Train Embedding 5100 saved.\n",
      "Train Embedding 5200 saved.\n",
      "Train Embedding 5300 saved.\n",
      "Train Embedding 5400 saved.\n",
      "Train Embedding 5500 saved.\n",
      "Train Embedding 5600 saved.\n",
      "Train Embedding 5700 saved.\n",
      "Train Embedding 5800 saved.\n",
      "Train Embedding 5900 saved.\n",
      "Train Embedding 6000 saved.\n",
      "Train Embedding 6100 saved.\n",
      "Train Embedding 6200 saved.\n",
      "Train Embedding 6300 saved.\n",
      "Train Embedding 6400 saved.\n",
      "Train Embedding 6500 saved.\n",
      "Train Embedding 6600 saved.\n",
      "Train Embedding 6700 saved.\n",
      "Train Embedding 6800 saved.\n",
      "Train Embedding 6900 saved.\n",
      "Train Embedding 7000 saved.\n",
      "Train Embedding 7100 saved.\n",
      "Train Embedding 7200 saved.\n",
      "Train Embedding 7300 saved.\n",
      "Train Embedding 7400 saved.\n",
      "Train Embedding 7500 saved.\n",
      "Train Embedding 7600 saved.\n",
      "Train Embedding 7700 saved.\n",
      "Train Embedding 7800 saved.\n",
      "Train Embedding 7900 saved.\n",
      "Train Embedding 8000 saved.\n",
      "Train Embedding 8100 saved.\n",
      "Train Embedding 8200 saved.\n",
      "Train Embedding 8300 saved.\n",
      "Train Embedding 8400 saved.\n",
      "Train Embedding 8500 saved.\n",
      "Train Embedding 8600 saved.\n",
      "Train Embedding 8700 saved.\n",
      "Train Embedding 8800 saved.\n",
      "Train Embedding 8900 saved.\n",
      "Train Embedding 9000 saved.\n",
      "Train Embedding 9100 saved.\n",
      "Train Embedding 9200 saved.\n",
      "Train Embedding 9300 saved.\n",
      "Train Embedding 9400 saved.\n",
      "Train Embedding 9500 saved.\n",
      "Train Embedding 9600 saved.\n",
      "Train Embedding 9700 saved.\n",
      "Train Embedding 9800 saved.\n",
      "Train Embedding 9900 saved.\n",
      "Train Embedding 10000 saved.\n",
      "Train Embedding 10100 saved.\n",
      "Train Embedding 10200 saved.\n",
      "Train Embedding 10300 saved.\n",
      "Train Embedding 10400 saved.\n",
      "Train Embedding 10500 saved.\n",
      "Train Embedding 10600 saved.\n",
      "Train Embedding 10700 saved.\n",
      "Train Embedding 10800 saved.\n",
      "Train Embedding 10900 saved.\n",
      "Train Embedding 11000 saved.\n",
      "Train Embedding 11100 saved.\n",
      "Train Embedding 11200 saved.\n",
      "Train Embedding 11300 saved.\n",
      "Train Embedding 11400 saved.\n",
      "Train Embedding 11500 saved.\n",
      "Train Embedding 11600 saved.\n",
      "Train Embedding 11700 saved.\n",
      "Train Embedding 11800 saved.\n",
      "Train Embedding 11900 saved.\n",
      "Train Embedding 12000 saved.\n",
      "Train Embedding 12100 saved.\n",
      "Train Embedding 12200 saved.\n",
      "Train Embedding 12300 saved.\n",
      "Train Embedding 12400 saved.\n",
      "Train Embedding 12500 saved.\n",
      "Train Embedding 12600 saved.\n",
      "Train Embedding 12700 saved.\n",
      "Train Embedding 12800 saved.\n",
      "Train Embedding 12900 saved.\n",
      "Train Embedding 13000 saved.\n",
      "Train Embedding 13100 saved.\n",
      "Train Embedding 13200 saved.\n",
      "Train Embedding 13300 saved.\n",
      "Train Embedding 13400 saved.\n",
      "Train Embedding 13500 saved.\n",
      "Train Embedding 13600 saved.\n",
      "Train Embedding 13700 saved.\n",
      "Train Embedding 13800 saved.\n",
      "Train Embedding 13900 saved.\n",
      "Train Embedding 14000 saved.\n",
      "Train Embedding 14100 saved.\n",
      "Train Embedding 14200 saved.\n",
      "Train Embedding 14300 saved.\n",
      "Train Embedding 14400 saved.\n",
      "Train Embedding 14500 saved.\n",
      "Train Embedding 14600 saved.\n",
      "Train Embedding 14700 saved.\n",
      "Train Embedding 14800 saved.\n",
      "Train Embedding 14900 saved.\n",
      "Train Embedding 15000 saved.\n",
      "Train Embedding 15100 saved.\n",
      "Train Embedding 15200 saved.\n",
      "Train Embedding 15300 saved.\n",
      "Train Embedding 15400 saved.\n",
      "Train Embedding 15500 saved.\n",
      "Train Embedding 15600 saved.\n",
      "Train Embedding 15700 saved.\n",
      "Train Embedding 15800 saved.\n",
      "Train Embedding 15900 saved.\n",
      "Train Embedding 16000 saved.\n",
      "Train Embedding 16100 saved.\n",
      "Train Embedding 16200 saved.\n",
      "Train Embedding 16300 saved.\n",
      "Train Embedding 16400 saved.\n",
      "Train Embedding 16500 saved.\n",
      "Train Embedding 16600 saved.\n",
      "Train Embedding 16700 saved.\n",
      "Train Embedding 16800 saved.\n",
      "Train Embedding 16900 saved.\n",
      "Train Embedding 17000 saved.\n",
      "Train Embedding 17100 saved.\n",
      "Train Embedding 17200 saved.\n",
      "Train Embedding 17300 saved.\n",
      "Train Embedding 17400 saved.\n",
      "Train Embedding 17500 saved.\n",
      "Train Embedding 17600 saved.\n",
      "Train Embedding 17700 saved.\n",
      "Train Embedding 17800 saved.\n",
      "Train Embedding 17900 saved.\n",
      "Train Embedding 18000 saved.\n",
      "Train Embedding 18100 saved.\n",
      "Train Embedding 18200 saved.\n",
      "Train Embedding 18300 saved.\n",
      "Train Embedding 18400 saved.\n",
      "Train Embedding 18500 saved.\n",
      "Train Embedding 18600 saved.\n",
      "Train Embedding 18700 saved.\n",
      "Train Embedding 18800 saved.\n",
      "Train Embedding 18900 saved.\n",
      "Train Embedding 19000 saved.\n",
      "Train Embedding 19100 saved.\n",
      "Train Embedding 19200 saved.\n",
      "Train Embedding 19300 saved.\n",
      "Train Embedding 19400 saved.\n",
      "Train Embedding 19500 saved.\n",
      "Train Embedding 19600 saved.\n",
      "Train Embedding 19700 saved.\n",
      "Train Embedding 19800 saved.\n",
      "Train Embedding 19900 saved.\n",
      "Train Embedding 20000 saved.\n",
      "Train Embedding 20100 saved.\n",
      "Train Embedding 20200 saved.\n",
      "Train Embedding 20300 saved.\n",
      "Train Embedding 20400 saved.\n",
      "Train Embedding 20500 saved.\n",
      "Train Embedding 20600 saved.\n",
      "Train Embedding 20700 saved.\n",
      "Train Embedding 20800 saved.\n",
      "Train Embedding 20900 saved.\n",
      "Train Embedding 21000 saved.\n",
      "Train Embedding 21100 saved.\n",
      "Train Embedding 21200 saved.\n",
      "Train Embedding 21300 saved.\n",
      "Train Embedding 21400 saved.\n",
      "Train Embedding 21500 saved.\n",
      "Train Embedding 21600 saved.\n",
      "Train Embedding 21700 saved.\n",
      "Train Embedding 21800 saved.\n",
      "Train Embedding 21900 saved.\n",
      "Train Embedding 22000 saved.\n",
      "Train Embedding 22100 saved.\n",
      "Train Embedding 22200 saved.\n",
      "Train Embedding 22300 saved.\n",
      "Train Embedding 22400 saved.\n",
      "Train Embedding 22500 saved.\n",
      "Train Embedding 22600 saved.\n",
      "Train Embedding 22700 saved.\n",
      "Train Embedding 22800 saved.\n",
      "Train Embedding 22900 saved.\n",
      "Train Embedding 23000 saved.\n",
      "Train Embedding 23100 saved.\n",
      "Train Embedding 23200 saved.\n",
      "Train Embedding 23300 saved.\n",
      "Train Embedding 23400 saved.\n",
      "Train Embedding 23500 saved.\n",
      "Train Embedding 23600 saved.\n",
      "Train Embedding 23700 saved.\n",
      "Train Embedding 23800 saved.\n",
      "Train Embedding 23900 saved.\n",
      "Train Embedding 24000 saved.\n",
      "Train Embedding 24100 saved.\n",
      "Train Embedding 24200 saved.\n",
      "Train Embedding 24300 saved.\n",
      "Train Embedding 24400 saved.\n",
      "Train Embedding 24500 saved.\n",
      "Train Embedding 24600 saved.\n",
      "Train Embedding 24700 saved.\n",
      "Train Embedding 24800 saved.\n",
      "Train Embedding 24900 saved.\n",
      "Train Embedding 25000 saved.\n",
      "Train Embedding 25100 saved.\n",
      "Train Embedding 25200 saved.\n",
      "Train Embedding 25300 saved.\n",
      "Train Embedding 25400 saved.\n",
      "Train Embedding 25500 saved.\n",
      "Train Embedding 25600 saved.\n",
      "Train Embedding 25700 saved.\n",
      "Train Embedding 25800 saved.\n",
      "Train Embedding 25900 saved.\n",
      "Train Embedding 26000 saved.\n",
      "Train Embedding 26100 saved.\n",
      "Train Embedding 26200 saved.\n",
      "Train Embedding 26300 saved.\n",
      "Train Embedding 26400 saved.\n",
      "Train Embedding 26500 saved.\n",
      "Train Embedding 26600 saved.\n",
      "Train Embedding 26700 saved.\n",
      "Train Embedding 26800 saved.\n",
      "Train Embedding 26900 saved.\n",
      "Train Embedding 27000 saved.\n",
      "Train Embedding 27100 saved.\n",
      "Train Embedding 27200 saved.\n",
      "Train Embedding 27300 saved.\n",
      "Train Embedding 27400 saved.\n",
      "Train Embedding 27500 saved.\n",
      "Train Embedding 27600 saved.\n",
      "Train Embedding 27700 saved.\n",
      "Train Embedding 27800 saved.\n",
      "Train Embedding 27900 saved.\n",
      "Train Embedding 28000 saved.\n",
      "Train Embedding 28100 saved.\n",
      "Train Embedding 28200 saved.\n",
      "Train Embedding 28300 saved.\n",
      "Train Embedding 28400 saved.\n",
      "Train Embedding 28500 saved.\n",
      "Train Embedding 28600 saved.\n",
      "Train Embedding 28700 saved.\n",
      "Train Embedding 28800 saved.\n",
      "Train Embedding 28900 saved.\n",
      "Train Embedding 29000 saved.\n",
      "Train Embedding 29100 saved.\n",
      "Train Embedding 29200 saved.\n",
      "Train Embedding 29300 saved.\n",
      "Train Embedding 29400 saved.\n",
      "Train Embedding 29500 saved.\n",
      "Train Embedding 29600 saved.\n",
      "Train Embedding 29700 saved.\n",
      "Train Embedding 29800 saved.\n",
      "Train Embedding 29900 saved.\n",
      "Train Embedding 30000 saved.\n",
      "Train Embedding 30100 saved.\n",
      "Train Embedding 30200 saved.\n",
      "Train Embedding 30300 saved.\n",
      "Train Embedding 30400 saved.\n",
      "Train Embedding 30500 saved.\n",
      "Train Embedding 30600 saved.\n",
      "Train Embedding 30700 saved.\n",
      "Train Embedding 30800 saved.\n",
      "Train Embedding 30900 saved.\n",
      "Train Embedding 31000 saved.\n",
      "Train Embedding 31100 saved.\n",
      "Train Embedding 31200 saved.\n",
      "Train Embedding 31300 saved.\n",
      "Train Embedding 31400 saved.\n",
      "Train Embedding 31500 saved.\n",
      "Train Embedding 31600 saved.\n",
      "Train Embedding 31700 saved.\n",
      "Train Embedding 31800 saved.\n",
      "Train Embedding 31900 saved.\n",
      "Train Embedding 32000 saved.\n",
      "Train Embedding 32100 saved.\n",
      "Train Embedding 32200 saved.\n",
      "Train Embedding 32300 saved.\n",
      "Train Embedding 32400 saved.\n",
      "Train Embedding 32500 saved.\n",
      "Train Embedding 32600 saved.\n",
      "Train Embedding 32700 saved.\n",
      "Train Embedding 32800 saved.\n",
      "Train Embedding 32900 saved.\n",
      "Train Embedding 33000 saved.\n",
      "Train Embedding 33100 saved.\n",
      "Train Embedding 33200 saved.\n",
      "Train Embedding 33300 saved.\n",
      "Train Embedding 33400 saved.\n",
      "Train Embedding 33500 saved.\n",
      "Train Embedding 33600 saved.\n",
      "Train Embedding 33700 saved.\n",
      "Train Embedding 33800 saved.\n",
      "Train Embedding 33900 saved.\n",
      "Train Embedding 34000 saved.\n",
      "Train Embedding 34100 saved.\n",
      "Train Embedding 34200 saved.\n",
      "Train Embedding 34300 saved.\n",
      "Train Embedding 34400 saved.\n",
      "Train Embedding 34500 saved.\n",
      "Train Embedding 34600 saved.\n",
      "Train Embedding 34700 saved.\n",
      "Train Embedding 34800 saved.\n",
      "Train Embedding 34900 saved.\n",
      "Train Embedding 35000 saved.\n",
      "Train Embedding 35100 saved.\n",
      "Train Embedding 35200 saved.\n",
      "Train Embedding 35300 saved.\n",
      "Train Embedding 35400 saved.\n",
      "Train Embedding 35500 saved.\n",
      "Train Embedding 35600 saved.\n",
      "Train Embedding 35700 saved.\n",
      "Train Embedding 35800 saved.\n",
      "Train Embedding 35900 saved.\n",
      "Train Embedding 36000 saved.\n",
      "Train Embedding 36100 saved.\n",
      "Train Embedding 36200 saved.\n",
      "Train Embedding 36300 saved.\n",
      "Train Embedding 36400 saved.\n",
      "Train Embedding 36500 saved.\n",
      "Train Embedding 36600 saved.\n",
      "Train Embedding 36700 saved.\n",
      "Train Embedding 36800 saved.\n",
      "Train Embedding 36900 saved.\n",
      "Train Embedding 37000 saved.\n",
      "Train Embedding 37100 saved.\n",
      "Train Embedding 37200 saved.\n",
      "Train Embedding 37300 saved.\n",
      "Train Embedding 37400 saved.\n",
      "Train Embedding 37500 saved.\n",
      "Train Embedding 37600 saved.\n",
      "Train Embedding 37700 saved.\n",
      "Train Embedding 37800 saved.\n",
      "Train Embedding 37900 saved.\n",
      "Train Embedding 38000 saved.\n",
      "Train Embedding 38100 saved.\n",
      "Train Embedding 38200 saved.\n",
      "Train Embedding 38300 saved.\n",
      "Train Embedding 38400 saved.\n",
      "Train Embedding 38500 saved.\n",
      "Train Embedding 38600 saved.\n",
      "Train Embedding 38700 saved.\n",
      "Train Embedding 38800 saved.\n",
      "Train Embedding 38900 saved.\n",
      "Train Embedding 39000 saved.\n",
      "Train Embedding 39100 saved.\n",
      "Train Embedding 39200 saved.\n",
      "Train Embedding 39300 saved.\n",
      "Train Embedding 39400 saved.\n",
      "Train Embedding 39500 saved.\n",
      "Train Embedding 39600 saved.\n",
      "Train Embedding 39700 saved.\n",
      "Train Embedding 39800 saved.\n",
      "Train Embedding 39900 saved.\n",
      "Train Embedding 40000 saved.\n",
      "Train Embedding 40100 saved.\n",
      "Train Embedding 40200 saved.\n",
      "Train Embedding 40300 saved.\n",
      "Train Embedding 40400 saved.\n",
      "Train Embedding 40500 saved.\n",
      "Train Embedding 40600 saved.\n",
      "Train Embedding 40700 saved.\n",
      "Train Embedding 40800 saved.\n",
      "Train Embedding 40900 saved.\n",
      "Train Embedding 41000 saved.\n",
      "Train Embedding 41100 saved.\n",
      "Train Embedding 41200 saved.\n",
      "Train Embedding 41300 saved.\n",
      "Train Embedding 41400 saved.\n",
      "Train Embedding 41500 saved.\n",
      "Train Embedding 41600 saved.\n",
      "Train Embedding 41700 saved.\n",
      "Train Embedding 41800 saved.\n",
      "Train Embedding 41900 saved.\n",
      "Train Embedding 42000 saved.\n",
      "Train Embedding 42100 saved.\n",
      "Train Embedding 42200 saved.\n",
      "Train Embedding 42300 saved.\n",
      "Train Embedding 42400 saved.\n",
      "Train Embedding 42500 saved.\n",
      "Train Embedding 42600 saved.\n",
      "Train Embedding 42700 saved.\n",
      "Train Embedding 42800 saved.\n",
      "Train Embedding 42900 saved.\n",
      "Train Embedding 43000 saved.\n",
      "Train Embedding 43100 saved.\n",
      "Train Embedding 43200 saved.\n",
      "Train Embedding 43300 saved.\n",
      "Train Embedding 43400 saved.\n",
      "Train Embedding 43500 saved.\n",
      "Train Embedding 43600 saved.\n"
     ]
    }
   ],
   "source": [
    "# Save embeddings for train, test and validation\n",
    "save_embeddings(poems_train, 'train', model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Embedding 0 saved.\n",
      "Val Embedding 100 saved.\n",
      "Val Embedding 200 saved.\n",
      "Val Embedding 300 saved.\n",
      "Val Embedding 400 saved.\n",
      "Val Embedding 500 saved.\n",
      "Val Embedding 600 saved.\n",
      "Val Embedding 700 saved.\n",
      "Val Embedding 800 saved.\n",
      "Val Embedding 900 saved.\n",
      "Val Embedding 1000 saved.\n",
      "Val Embedding 1100 saved.\n",
      "Val Embedding 1200 saved.\n",
      "Val Embedding 1300 saved.\n",
      "Val Embedding 1400 saved.\n",
      "Val Embedding 1500 saved.\n",
      "Val Embedding 1600 saved.\n",
      "Val Embedding 1700 saved.\n",
      "Val Embedding 1800 saved.\n",
      "Val Embedding 1900 saved.\n",
      "Val Embedding 2000 saved.\n",
      "Val Embedding 2100 saved.\n",
      "Val Embedding 2200 saved.\n",
      "Val Embedding 2300 saved.\n",
      "Val Embedding 2400 saved.\n",
      "Val Embedding 2500 saved.\n",
      "Val Embedding 2600 saved.\n",
      "Val Embedding 2700 saved.\n",
      "Val Embedding 2800 saved.\n",
      "Val Embedding 2900 saved.\n",
      "Val Embedding 3000 saved.\n",
      "Val Embedding 3100 saved.\n",
      "Val Embedding 3200 saved.\n",
      "Val Embedding 3300 saved.\n",
      "Val Embedding 3400 saved.\n",
      "Val Embedding 3500 saved.\n",
      "Val Embedding 3600 saved.\n",
      "Val Embedding 3700 saved.\n",
      "Val Embedding 3800 saved.\n",
      "Val Embedding 3900 saved.\n",
      "Val Embedding 4000 saved.\n",
      "Val Embedding 4100 saved.\n",
      "Val Embedding 4200 saved.\n",
      "Val Embedding 4300 saved.\n",
      "Val Embedding 4400 saved.\n",
      "Val Embedding 4500 saved.\n",
      "Val Embedding 4600 saved.\n",
      "Val Embedding 4700 saved.\n",
      "Val Embedding 4800 saved.\n",
      "Val Embedding 4900 saved.\n",
      "Val Embedding 5000 saved.\n",
      "Val Embedding 5100 saved.\n",
      "Val Embedding 5200 saved.\n",
      "Val Embedding 5300 saved.\n",
      "Val Embedding 5400 saved.\n"
     ]
    }
   ],
   "source": [
    "save_embeddings(poems_val, 'val', model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Embedding 0 saved.\n",
      "Test Embedding 100 saved.\n",
      "Test Embedding 200 saved.\n",
      "Test Embedding 300 saved.\n",
      "Test Embedding 400 saved.\n",
      "Test Embedding 500 saved.\n",
      "Test Embedding 600 saved.\n",
      "Test Embedding 700 saved.\n",
      "Test Embedding 800 saved.\n",
      "Test Embedding 900 saved.\n",
      "Test Embedding 1000 saved.\n",
      "Test Embedding 1100 saved.\n",
      "Test Embedding 1200 saved.\n",
      "Test Embedding 1300 saved.\n",
      "Test Embedding 1400 saved.\n",
      "Test Embedding 1500 saved.\n",
      "Test Embedding 1600 saved.\n",
      "Test Embedding 1700 saved.\n",
      "Test Embedding 1800 saved.\n",
      "Test Embedding 1900 saved.\n",
      "Test Embedding 2000 saved.\n",
      "Test Embedding 2100 saved.\n",
      "Test Embedding 2200 saved.\n",
      "Test Embedding 2300 saved.\n",
      "Test Embedding 2400 saved.\n",
      "Test Embedding 2500 saved.\n",
      "Test Embedding 2600 saved.\n",
      "Test Embedding 2700 saved.\n",
      "Test Embedding 2800 saved.\n",
      "Test Embedding 2900 saved.\n",
      "Test Embedding 3000 saved.\n",
      "Test Embedding 3100 saved.\n",
      "Test Embedding 3200 saved.\n",
      "Test Embedding 3300 saved.\n",
      "Test Embedding 3400 saved.\n",
      "Test Embedding 3500 saved.\n",
      "Test Embedding 3600 saved.\n",
      "Test Embedding 3700 saved.\n",
      "Test Embedding 3800 saved.\n",
      "Test Embedding 3900 saved.\n",
      "Test Embedding 4000 saved.\n",
      "Test Embedding 4100 saved.\n",
      "Test Embedding 4200 saved.\n",
      "Test Embedding 4300 saved.\n",
      "Test Embedding 4400 saved.\n",
      "Test Embedding 4500 saved.\n",
      "Test Embedding 4600 saved.\n",
      "Test Embedding 4700 saved.\n",
      "Test Embedding 4800 saved.\n",
      "Test Embedding 4900 saved.\n",
      "Test Embedding 5000 saved.\n",
      "Test Embedding 5100 saved.\n",
      "Test Embedding 5200 saved.\n",
      "Test Embedding 5300 saved.\n",
      "Test Embedding 5400 saved.\n"
     ]
    }
   ],
   "source": [
    "save_embeddings(poems_test, 'test', model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = sorted(list(set(lables_train)))\n",
    "\n",
    "def load_embeddings(prefix, num_embeddings):\n",
    "    embeddings = []\n",
    "    for i in range(num_embeddings):\n",
    "        embedding = torch.load(f'./models_roberta_urdu/{prefix}_embedding_{i}.pt')\n",
    "        embeddings.append(embedding)\n",
    "    return torch.stack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14723/1025018291.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embedding = torch.load(f'./models_roberta_urdu/{prefix}_embedding_{i}.pt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7200, -0.0091,  0.1667,  ...,  0.3693,  1.6527, -0.0542]],\n",
       "\n",
       "        [[ 0.8675,  0.2121,  0.2090,  ...,  0.0997,  1.1922, -0.3499]],\n",
       "\n",
       "        [[ 0.6282, -0.2588, -0.1849,  ...,  0.5559,  1.3151, -0.2796]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.7985,  0.5469,  0.0372,  ...,  0.4290,  1.6407, -0.1550]],\n",
       "\n",
       "        [[ 0.7071,  0.5777,  0.0671,  ...,  0.6072,  1.5448, -0.0921]],\n",
       "\n",
       "        [[ 0.5011, -0.2120,  0.0715,  ...,  0.2596,  1.7465, -0.2179]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings = load_embeddings('train', len(poems_train))\n",
    "train_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14723/1025018291.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embedding = torch.load(f'./models_roberta_urdu/{prefix}_embedding_{i}.pt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8267, -0.2748,  0.4003,  ...,  0.3170,  1.0390, -0.0601]],\n",
       "\n",
       "        [[ 0.7834, -0.0339, -0.0681,  ...,  0.4437,  1.3576, -0.0454]],\n",
       "\n",
       "        [[ 0.8073,  0.4141,  0.1188,  ...,  0.6287,  1.4653, -0.2032]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.5644, -0.0054,  0.0542,  ...,  0.2336,  1.6979, -0.2195]],\n",
       "\n",
       "        [[ 0.5286,  0.0336,  0.0806,  ...,  0.2093,  1.7248, -0.2653]],\n",
       "\n",
       "        [[ 0.5498,  0.1596, -0.0471,  ...,  0.7972,  1.1110, -0.3658]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_embeddings = load_embeddings('val', len(poems_val))\n",
    "val_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14723/1025018291.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embedding = torch.load(f'./models_roberta_urdu/{prefix}_embedding_{i}.pt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5880, -0.1590,  0.0435,  ...,  0.0070,  1.6093, -0.3489]],\n",
       "\n",
       "        [[ 0.5811, -0.1360, -0.0797,  ...,  0.6468,  1.3318, -0.2723]],\n",
       "\n",
       "        [[ 0.8738, -0.1279, -0.0895,  ...,  0.5300,  1.4722, -0.3882]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.1735,  0.4458,  0.2705,  ...,  0.8301,  1.1468, -0.0167]],\n",
       "\n",
       "        [[ 0.7532,  0.2574,  0.0437,  ...,  0.5354,  1.6378, -0.0621]],\n",
       "\n",
       "        [[ 0.4559, -0.1600,  0.1205,  ...,  0.1971,  1.7472, -0.1388]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embeddings = load_embeddings('test', len(poems_test))\n",
    "test_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(lables_train)\n",
    "val_labels = torch.tensor(labels_val)\n",
    "test_labels = torch.tensor(labels_test)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "# encoded_labels = label_encoder.fit_transform(lables_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_np = train_embeddings.cpu().detach().numpy()\n",
    "val_embeddings_np = val_embeddings.cpu().detach().numpy()\n",
    "test_embeddings_np = test_embeddings.cpu().detach().numpy()\n",
    "labels_train_np = np.array(lables_train)\n",
    "labels_val_np = np.array(labels_val)\n",
    "labels_test_np = np.array(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((43696, 1, 768), (5462, 1, 768), (5462, 1, 768), (43696,), (5462,), (5462,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings_np.shape, val_embeddings_np.shape, test_embeddings_np.shape, labels_train_np.shape, labels_val_np.shape, labels_test_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT WITH BERT\n",
    "\n",
    "class PoemClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PoemClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten along the sequence length dimension\n",
    "        x = F.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the input size based on the size of the embeddings\n",
    "input_size = train_embeddings.size(2)\n",
    "\n",
    "# initialize the classifier and move it to the device\n",
    "classifier = PoemClassifier(input_size, len(labels)).to(device)\n",
    "# classifier = PoemClassifier(input_size, len(class_labels)).to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    classifier = nn.DataParallel(classifier)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create dataloader for training to iterate over the embeddings and labels in batches\n",
    "train_dataset = TensorDataset(train_embeddings, train_labels)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accuracies = []\n",
    "train_accuracies = []\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # You can adjust this based on your needs\n",
    "counter = 0\n",
    "train_losses = []  # Track training loss\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_embeddings, val_labels), batch_size = 8)\n",
    "accumalation_steps = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7244301ff6e0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x72440bff0890>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Loop with early stopping\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    classifier.train()\n",
    "    epoch_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, (batch_embeddings, batch_labels) in enumerate(train_loader):\n",
    "        if i % 100 == 0: print(f\"Iteration {i}/{len(train_loader)} of train loader\")\n",
    "        batch_embeddings, batch_labels = batch_embeddings.to(device), batch_labels.to(device)\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "        output = classifier(batch_embeddings)\n",
    "        loss = criterion(output, batch_labels)\n",
    "        # loss.backward()\n",
    "        loss = loss / accumalation_steps\n",
    "        loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        if (i + 1) % accumalation_steps == 0 or (i + 1) == len(train_loader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            epoch_loss += loss.item() * accumalation_steps\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader.dataset)\n",
    "    train_losses.append(avg_epoch_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Training and Validation Accuracy:\")\n",
    "\n",
    "    # Training accuracy and data in batches to avoid over memory consumption\n",
    "    classifier.eval()\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_embeddings, batch_labels in train_loader:\n",
    "            batch_embeddings, batch_labels = batch_embeddings.to(device), batch_labels.to(device)\n",
    "            train_outputs = classifier(batch_embeddings)\n",
    "            train_probabilities = F.softmax(train_outputs, dim=1)\n",
    "            train_predicted_labels = torch.argmax(train_probabilities, dim=1)\n",
    "            train_correct += (train_predicted_labels == batch_labels).sum().item()\n",
    "            train_total += batch_labels.size(0)\n",
    "    train_accuracy = train_correct / train_total\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "\n",
    "    # Validation accuracy and loss in batches to avoid over memory consumption\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_loss_total = 0\n",
    "    with torch.no_grad():\n",
    "        for val_batch_embeddings, val_batch_labels in DataLoader(TensorDataset(val_embeddings, val_labels), batch_size=8):\n",
    "            val_batch_embeddings, val_batch_labels = val_batch_embeddings.to(device), val_batch_labels.to(device)\n",
    "            val_outputs = classifier(val_batch_embeddings)\n",
    "            val_loss = criterion(val_outputs, val_batch_labels)\n",
    "            val_loss_total += val_loss.item()\n",
    "\n",
    "            val_probabilities = F.softmax(val_outputs, dim=1)\n",
    "            val_predicted_labels = torch.argmax(val_probabilities, dim=1)\n",
    "            val_correct += (val_predicted_labels == val_batch_labels).sum().item()\n",
    "            val_total += val_batch_labels.size(0)\n",
    "\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_losses.append(val_loss_total / len(val_loader))\n",
    "        \n",
    "\n",
    "    # Print current status\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_epoch_loss:.4f}, Val Loss: {val_loss_total / len(val_loader):.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss_total < best_val_loss:\n",
    "        best_val_loss = val_loss_total\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early Stopping!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation on the test set\n",
    "with torch.no_grad():\n",
    "    classifier.eval()\n",
    "    test_outputs = classifier(test_embeddings.to(device))\n",
    "    test_loss = criterion(test_outputs, test_labels.to(device))\n",
    "\n",
    "    # convert logits to probs\n",
    "    test_probabilities = F.softmax(test_outputs, dim=1)\n",
    "\n",
    "    # get the predicted labels\n",
    "    test_predicted_labels = torch.argmax(test_probabilities, dim=1).tolist()\n",
    "\n",
    "    cm = confusion_matrix(test_labels, test_predicted_labels)\n",
    "\n",
    "    # calculate the accuracy\n",
    "    test_accuracy = accuracy_score(test_labels, test_predicted_labels)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    test_loss_value = test_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(labels_test_np, test_predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot accuracy and loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Training Accuracy')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 3, 3)\n",
    "plt.bar([\"Test Loss\"], [test_loss_value], color=\"skyblue\")\n",
    "plt.title(\"Test Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
