{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/.local/lib/python3.12/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import os, joblib\n",
    "import xgboost as xgb\n",
    "import pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.utils import resample\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.ensemble import BalancedRandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abbasinYousuf': 0, 'ajmalKhattak': 1, 'allamaAbdulHai': 2, 'ghaniKhan': 3, 'hamzaBaba': 4, 'javedAhmedzai': 5, 'karanKhan': 6, 'khatirAfridi': 7, 'khushalKhanKhattak': 8, 'matiullahTurab': 9, 'mumtazOrakazi': 10, 'munirJan': 11, 'naeemAhmed': 12, 'rabiaMumtaz': 13, 'rahmanBaba': 14, 'rehmatShah': 15, 'sahibShahSabir': 16, 'shabbirKhanDurrani': 17, 'shakirOrakzai': 18, 'shoaibKhanKhattak': 19}\n"
     ]
    }
   ],
   "source": [
    "# Define Data directory\n",
    "DATA_DIR = '../data/'\n",
    "\n",
    "# list of poet names\n",
    "# Removed the 5 lowest poets\n",
    "poets = [\"abbasinYousuf\", \"ajmalKhattak\", \"allamaAbdulHai\", \"ghaniKhan\", \"hamzaBaba\", \"javedAhmedzai\", \"karanKhan\", \"khatirAfridi\", \"khushalKhanKhattak\", \"matiullahTurab\", \"mumtazOrakazi\", \"munirJan\", \"naeemAhmed\", \"rabiaMumtaz\", \"rahmanBaba\", \"rehmatShah\", \"sahibShahSabir\", \"shabbirKhanDurrani\", \"shakirOrakzai\", \"shoaibKhanKhattak\"]\n",
    "\n",
    "# # Pehle k 13\n",
    "# poets = [\"abbasinYousuf\", \"azizMazerwal\", \"ghaniKhan\", \"hamzaBaba\", \"khaliqZiari\", \"khatirAfridi\", \"khushalKhanKhattak\",  \"mumtazOrakazi\", \"munirJan\",  \"rahmanBaba\", \"rehmatShah\", \"sahibShahSabir\", \"salimRiaz\"]\n",
    "\n",
    "## All\n",
    "# poets = [\"abbasinYousuf\", \"azizMazerwal\", \"ghaniKhan\", \"hamzaBaba\", \"karanKhan\", \"khaliqZiari\", \"khatirAfridi\", \"khushalKhanKhattak\", \"matiullahTurab\", \"mumtazOrakazi\", \"munirJan\", \"naeemAhmed\", \"rabiaMumtaz\", \"rahmanBaba\", \"rehmatShah\", \"sahibShahSabir\", \"salimRiaz\"]\n",
    "\n",
    "poet_labels = {poet: i for i, poet in enumerate(poets)}\n",
    "print(poet_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Label the Data\n",
    "def load_and_label(data_dir, poets, poet_labels):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for poet in poets:\n",
    "        poet_dir = os.path.join(data_dir, poet)\n",
    "        file_path = os.path.join(poet_dir, f'{poet}.txt')\n",
    "\n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f'{file_path} does not exist')\n",
    "            continue\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            data.extend(lines)\n",
    "            labels.extend([poet_labels[poet]] * len(lines))\n",
    "\n",
    "    df = pd.DataFrame({'text': data, 'label': labels})\n",
    "    return data, labels, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>54620.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.625265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.352617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label\n",
       "count  54620.000000\n",
       "mean       8.625265\n",
       "std        5.352617\n",
       "min        0.000000\n",
       "25%        4.000000\n",
       "50%        8.000000\n",
       "75%       14.000000\n",
       "max       19.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, labels, df = load_and_label(DATA_DIR, poets, poet_labels)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(df):\n",
    "    max_size = df['label'].value_counts().max()\n",
    "    lst = [df]\n",
    "    for class_index, group in df.groupby('label'):\n",
    "        lst.append(group.sample(max_size-len(group), replace=True))\n",
    "    df_balanced = pd.concat(lst)\n",
    "    return df_balanced.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = balance_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_train_test_split(df, test_size=0.2, random_state=42):\n",
    "    train_dfs = []\n",
    "    test_dfs = []\n",
    "\n",
    "    # Split each poet's data individually\n",
    "    for poet_label in df['label'].unique():\n",
    "        poet_df = df[df['label'] == poet_label]\n",
    "        train_poet, test_poet = train_test_split(\n",
    "            poet_df, test_size=test_size, random_state=random_state, stratify=poet_df['label']\n",
    "        )\n",
    "        train_dfs.append(train_poet)\n",
    "        test_dfs.append(test_poet)\n",
    "\n",
    "    # Concatenate all individual splits to get the final training and testing dataframes\n",
    "    train_df = pd.concat(train_dfs).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    test_df = pd.concat(test_dfs).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified extract_features function\n",
    "def extract_features(df, tfidf_vectorizer=None, num_features=None):\n",
    "    if tfidf_vectorizer is None:\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=num_features)\n",
    "        X = tfidf_vectorizer.fit_transform(df['text']).toarray()\n",
    "    else:\n",
    "        X = tfidf_vectorizer.transform(df['text']).toarray()\n",
    "    y = df['label'].values\n",
    "    return X, y, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df, num_features=None):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=num_features)\n",
    "    # tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X = tfidf_vectorizer.fit_transform(df['text']).toarray()\n",
    "    y = df['label'].values\n",
    "    return X, y, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, test_df = weighted_train_test_split(df, test_size=0.2)\n",
    "# X_train, y_train, tfidf_vectorizer = extract_features(train_df, 5000)\n",
    "# X_test, y_test, _ = extract_features(test_df, 5000)\n",
    "# train_df, test_df = weighted_train_test_split(df_balanced, test_size=0.2)\n",
    "train_df, test_df = weighted_train_test_split(df, test_size=0.2)\n",
    "X_train, y_train, tfidf_vectorizer = extract_features(train_df, num_features=10000)  # Fit vectorizer on training data\n",
    "X_test, y_test, _ = extract_features(test_df, tfidf_vectorizer=tfidf_vectorizer)  # Use same vectorizer for test data\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the split results\n",
    "print(f\"Training set size: {len(train_df)} samples\")\n",
    "print(f\"Testing set size: {len(test_df)} samples\")\n",
    "print(f\"Class distribution in training set:\\n{train_df['label'].value_counts(normalize=True)}\")\n",
    "print(f\"Class distribution in testing set:\\n{test_df['label'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smote = SMOTE(random_state=42)\n",
    "# X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y, tfidf_vectorizer = extract_features(df)\n",
    "# X, y, tfidf_vectorizer = extract_features(df, 15000)\n",
    "# X, y, tfidf_vectorizer = extract_features(df, 5000)\n",
    "X, y, tfidf_vectorizer = extract_features(df_balanced, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 170280\n",
      "Number of features (TF-IDF): 5000\n",
      "Class distribution:\n",
      "label\n",
      "15    8514\n",
      "19    8514\n",
      "9     8514\n",
      "11    8514\n",
      "16    8514\n",
      "17    8514\n",
      "18    8514\n",
      "4     8514\n",
      "1     8514\n",
      "7     8514\n",
      "3     8514\n",
      "5     8514\n",
      "10    8514\n",
      "6     8514\n",
      "14    8514\n",
      "12    8514\n",
      "13    8514\n",
      "0     8514\n",
      "2     8514\n",
      "8     8514\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Number of samples: {len(df)}\")\n",
    "# print(f\"Number of features (TF-IDF): {X.shape[1]}\")\n",
    "# print(f\"Class distribution:\\n{df['label'].value_counts()}\")\n",
    "print(f\"Number of samples: {len(df_balanced)}\")\n",
    "print(f\"Number of features (TF-IDF): {X.shape[1]}\")\n",
    "print(f\"Class distribution:\\n{df_balanced['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((136224, 5000), (34056, 5000), (136224,), (34056,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Random Forest model...\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42)\n",
    "# rf = RandomForestClassifier(n_estimators=1000, max_depth=None, random_state=42)\n",
    "\n",
    "print(f\"Training the Random Forest model...\")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(f\"Training and Predicting Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "conf_mat_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "# Print Metrics\n",
    "print(\"\\nModel Performance and Evaluation\")\n",
    "# print(f\"Accuracy: {accuracy_rf:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_rf*100:.2f}%\")\n",
    "print(f\"F1 Score (Weighted): {f1_rf:.4f}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_rf)}\")\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat_rf, annot=True, fmt='d', xticklabels=poets, yticklabels=poets)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Model\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Training and predictions\n",
    "print(f\"Training the SVM model...\")\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "print(f\"Training and Predicting Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "f1_svm = f1_score(y_test, y_pred_svm, average='weighted')\n",
    "conf_mat_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "# Print Metrics\n",
    "print(\"\\nModel Performance and Evaluation\")\n",
    "# print(f\"Accuracy: {accuracy_svm:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_svm*100:.2f}%\")\n",
    "print(f\"F1 Score (Weighted): {f1_svm:.4f}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_svm)}\")\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat_svm, annot=True, fmt='d', xticklabels=poets, yticklabels=poets)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix - SVM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Model\n",
    "svm = SVC(C=1.0, kernel='rbf', gamma='scale', probability=True, random_state=42)\n",
    "\n",
    "# Training and predictions\n",
    "print(f\"Training the SVM model...\")\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "print(f\"Training and Predicting Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "f1_svm = f1_score(y_test, y_pred_svm, average='weighted')\n",
    "conf_mat_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "# Print Metrics\n",
    "print(\"\\nModel Performance and Evaluation\")\n",
    "# print(f\"Accuracy: {accuracy_svm:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_svm*100:.2f}%\")\n",
    "print(f\"F1 Score (Weighted): {f1_svm:.4f}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_svm)}\")\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat_svm, annot=True, fmt='d', xticklabels=poets, yticklabels=poets)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix - SVM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "print(f\"Training the Logistic Regression model...\")\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "print(f\"Training and Predicting Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "f1_log_reg = f1_score(y_test, y_pred_log_reg, average='weighted')\n",
    "conf_mat_log_reg = confusion_matrix(y_test, y_pred_log_reg)\n",
    "\n",
    "# Print Metrics\n",
    "print(\"\\nModel Performance and Evaluation\")\n",
    "print(f\"Accuracy: {accuracy_log_reg:.4f}%\")\n",
    "print(f\"F1 Score (Weighted): {f1_log_reg:.4f}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_log_reg)}\")\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat_log_reg, annot=True, fmt='d', xticklabels=poets, yticklabels=poets)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "\n",
    "print(f\"Training the Logistic Regression model...\")\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "print(f\"Training and Predicting Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "f1_log_reg = f1_score(y_test, y_pred_log_reg, average='weighted')\n",
    "conf_mat_log_reg = confusion_matrix(y_test, y_pred_log_reg)\n",
    "\n",
    "# Print Metrics\n",
    "print(\"\\nModel Performance and Evaluation\")\n",
    "print(f\"Accuracy: {accuracy_log_reg:.4f}%\")\n",
    "print(f\"F1 Score (Weighted): {f1_log_reg:.4f}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_log_reg)}\")\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat_log_reg, annot=True, fmt='d', xticklabels=poets, yticklabels=poets)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(C=10, max_iter=1000, solver='newton-cg')\n",
    "\n",
    "print(f\"Training the Logistic Regression model...\")\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "print(f\"Training and Predicting Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "f1_log_reg = f1_score(y_test, y_pred_log_reg, average='weighted')\n",
    "conf_mat_log_reg = confusion_matrix(y_test, y_pred_log_reg)\n",
    "\n",
    "# Print Metrics\n",
    "print(\"\\nModel Performance and Evaluation\")\n",
    "print(f\"Accuracy: {accuracy_log_reg:.4f}%\")\n",
    "print(f\"F1 Score (Weighted): {f1_log_reg:.4f}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_log_reg)}\")\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat_log_reg, annot=True, fmt='d', xticklabels=poets, yticklabels=poets)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1 max_depth=3, random_state=42)\n",
    "\n",
    "print(f\"Training the Gradient Boosting model...\")\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "print(f\"Training and Predicting Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "f1_gb = f1_score(y_test, y_pred_gb, average='weighted')\n",
    "conf_mat_gb = confusion_matrix(y_test, y_pred_gb)\n",
    "\n",
    "# Print Metrics\n",
    "print(\"\\nModel Performance and Evaluation\")\n",
    "print(f\"Accuracy: {accuracy_gb:.4f}%\")\n",
    "print(f\"F1 Score (Weighted): {f1_gb:.4f}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_gb)}\")\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat_gb, annot=True, fmt='d', xticklabels=poets, yticklabels=poets)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix - Gradient Boosting')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost = xgb.XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "xgboost = xgb.XGBClassifier(scale_pos_weight=1, n_estimators=100, learning_rate=0.1, max_depth=3, gamma=0.1, eval_metric='logloss', random_state=42)\n",
    "\n",
    "print(f\"Training the XGBoost model...\")\n",
    "xgboost.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgboost = xgboost.predict(X_test)\n",
    "print(f\"Training and Predicting Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "accuracy_xgboost = accuracy_score(y_test, y_pred_xgboost)\n",
    "f1_xgboost = f1_score(y_test, y_pred_xgboost, average='weighted')\n",
    "conf_mat_xgboost = confusion_matrix(y_test, y_pred_xgboost)\n",
    "\n",
    "# Print Metrics\n",
    "print(\"\\nModel Performance and Evaluation\")\n",
    "print(f\"Accuracy: {accuracy_xgboost:.4f}%\")\n",
    "print(f\"F1 Score (Weighted): {f1_xgboost:.4f}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_xgboost)}\")\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat_xgboost, annot=True, fmt='d', xticklabels=poets, yticklabels=poets)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix - XGBoost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost = xgb.XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "xgboost = xgb.XGBClassifier(objective='multi:softmax', scale_pos_weight=1, n_estimators=100, learning_rate=0.1, max_depth=3, gamma=0.1, eval_metric='logloss', random_state=42)\n",
    "\n",
    "print(f\"Training the XGBoost model...\")\n",
    "xgboost.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgboost = xgboost.predict(X_test)\n",
    "print(f\"Training and Predicting Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "accuracy_xgboost = accuracy_score(y_test, y_pred_xgboost)\n",
    "f1_xgboost = f1_score(y_test, y_pred_xgboost, average='weighted')\n",
    "conf_mat_xgboost = confusion_matrix(y_test, y_pred_xgboost)\n",
    "\n",
    "# Print Metrics\n",
    "print(\"\\nModel Performance and Evaluation\")\n",
    "print(f\"Accuracy: {accuracy_xgboost:.4f}%\")\n",
    "print(f\"F1 Score (Weighted): {f1_xgboost:.4f}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_xgboost)}\")\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat_xgboost, annot=True, fmt='d', xticklabels=poets, yticklabels=poets)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix - XGBoost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
