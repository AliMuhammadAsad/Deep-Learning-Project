\section{Related Work}

Poet attribution using deep learning and machine learning has been explored in various languages previously as well. 

A recent study on the poet attribution for Punjabi poetry \cite{fatimarazapunjabipoetry} used deep learning, including transformer-based techniques for three different scripts of punjabi poetry; Gurmukhi, Shahmukhi, and Roman scripts. They curate a dataset from scratch, consisting of 830 poems from 11 punjabi poets. Their methodology includes splitting the data into 80-10-10 for training, validation, and testing, respectively, followed by tokenization and embedding generation which is then passed onto the deep learning and transformer models. They also experiment with TF-IDF vectorization instead of embeddings for the models. The trained Bi-LSTM, Bi-GRU, DistilBERT, Softmax regression, SVM, and Random Forest. The DistilBERT gave the highest accuracy for the Gurmukhi and Shahmukhi script of 90.36\% and 87.95\% respectively. The Bi-Gru follows with an accuracy of 89.16\% on the Gurmukhi script, while Bi-LSTM have the second highest accuracy of 81.93\% on the Shahmukhi script. Bi-LSTM performed the best on the roman script with an accuracy of 91.57\%, followed by SVM with an 89.16\% accuracy, and then DistilBERT with an accuracy of 87.95\%. 

A study on poet attribution for Urdu Ghazals \cite{fizza_iqra_urdu_attribution} employed machine learning, deep learning, and transformer-based techniques on a dataset of 18,609 couplets from 15 notable Urdu poets, covering both the 18th-19th and 19th-20th centuries. The distinct word usage and styles across these periods benefited the model in identifying patterns. The study tested four machine learning models — Logistic Regression, Random Forest, Naive Bayes, and SVM — using label encodings and TF-IDF for feature extraction. They also experimented with four deep learning models: Flatten, LSTM, GRU, and 1D-CNN, utilizing one-hot encoding and tokenization with a 300-dimensional embedding layer. The best performance came from transformer models: Bert and roBerta achieved 80.32\% and 79.52\% accuracy, respectively, outperforming deep learning methods. Among traditional machine learning models, SVM and Logistic Regression achieved 64\% and 60\% accuracy, surpassing deep learning approaches, where LSTM performed best at 59.96\%. The study notes class imbalance as a challenge but emphasizes the superior performance of transformer models in poet attribution tasks.

We also came across a study on Urdu Ghazal generation using Deep Learning models \cite{sandypaper} on a dataset of 17,609 Urdu couplets from 15 notable poets. They employed three different approaches; the n-gram probabilistic model, deep learning models such as LSTM and GRU, and state of the art GPT2 model. They evaluated their models using BLEU scores, as well as rhyme scores, and human evaluations. Their n-gram model gave the highest BLEU score of 0.5, closely followed by the GPT2 model which gave a BLEU score of 0.48 for Urdu for face, while for Urdu for Awaz (sound), the GPT model gave a score of 0.6 followed by n-gram with a score of 0.4. However, both the LSTM and GRU models suffered from overfitting, giving an accuracy of 28\% and 32\% respectively.

Another interesting study we came across was to classify Urdu music using convolutional neural networks, another deep learning technique \cite{shayanshafaqmusicclassification}. They identified 4 genres for their purposes; Rock, Hiphop, Qawwali, and Ghazal. They curated a dataset of 1000 samples, with 250 samples for each genre. The models they implemented included CNNs, XGBoost, and Logistic Regression, evaluating their models on accuracy and loss. Their CNN implementation first included a \textit{Mel Frequency Cepstral Co-efficient Generation}, which gives a 2D representation of the audio signals. Then those signals and features were passed on through convolutional layers, pooling layers, batch normalizations, falttening layers followed by dense layers. Using batch normalization, they achieved an accuracy of 92.6\%, and a loss of 0.0051 which was the best among all their models and experimentations. A CNN with global average pooling got the second best results with an accuracy of 83.1\%. The Logistic Regression model gave an accuracy of 82.7\%, while the XGBoost gave an accuracy of 81.8\%.

Another study was conducted on the classification of Persian poetry based on the poet's era \cite{persian_poetry_classification}. The authors used the Persian Hafez's Ghazal dataset which contained 496 Persian Ghazals. The authors categorized Persian Ghazals based on the poetic era in which they were written, with the classification focusing on distinguishiing Hafez's ghazals by the time period or era associated with the poetry using sequential learning architectures. Word embeddings were used, an important technique in NLP, and deep learning classification models. In addition, they also used Bag Of Words (BOW) - a baseline statistical model for textual feature extraction, and Latent Dirichlet Allocation (LDA) for feature extraction. They also used Distributed Memory (DM) for vectorization, and concatenated Distributed BOW with DM. Then they trained Machine Learning (ML) and Deep Learning (DL) models including Random Forest, Logistic Regression, LSTMs, GRU, and Bi-LSTM. For evaluation, they used accuracy, F1 score, precision, and recall to evaluate the performance of their models. The ML models were compared to SVM, which performed better in terms of accuracy but not F1 score. In the DL models, LSTM gave highest accuracy of 77.8\% and the highest F1-score of 76.6\% on the Persian dataset.  

We also came across a study that classified Gujrati Poetry based on emotions using deep learning techniques \cite{gujrati_poetry_attribution}. The author collected the first Gujrati poetry dataset of more than 300 different Gujrati poems, and called it the ``Kavan'' dataset that represented the different ``Rasa's'' emotions. They further used the NLTK library in Python for tokenization and labelling of the data. Poems were used as input one by one, with each word compared to a metadata based on the ``Navarasa'' concept, returning values between 0 and 8 for the emotions. They implemented a Deep Learning classifier model, and achieved an 87.62\% accuracy. 

In addition to classification and attribution, another study aimed to perform a comparative analysis on various machine learning models for Urdu poet attribution \cite{urdu_poet_attribution_kiet}. They collected a total of 1563 poems from 5 famous Urdu poets. They also tokenized the data using Python's NLTK library, however, they did not remove stop words as they believed that such functions are important in modeling the style of the author. They repeated various experiments on various models repeatedly with different parameters, to conclude that SVM performed the best over most configurations with the highest accuracy of 88.7\% and an average accuracy of 81\%, with Naive Bayes at a close second with the highest accuracy of 84\% and an average accuracy of 77\%. They also implemented LSTM models, however, the highest accuracy they received was 45.78\%. They concluded that with over 100 samples per poet and more than 2 lines per sample, one could achieve good results in poet attribution. 

Regarding Urdu language we found another research on the poet attribution on urdu authorship of poets \cite{urdu_authorship_attribution}. They collected a total of 11406 couplets from nonsocial Urdu websites of mainly 3 different Urdu poets. They developed various models for classification including SVM, Multilayer Perceptron (MLP), Multinomial Naive Bayes, and Multinomial MLP Pre-Trained Word2Vec model. SVMs achieved the highest accuracy and F1-score of 82.85\% and 82.67\%. 

Another similar work was done on identification of Urdu Ghazals using SVM \cite{urdu_ghazal_svm}. The authors collected a total of 3967 couplets from 4 poets, and generated a total of one million tokens from the dataset with a vocabulary size of 6427. They also created a Term-Document Frequency (TDF) matrix with rows representing couplets and columns representing unique terms. For feature selection, they used Chi-Square and L1-based techniques to select the best features. The Chi-Square evaluates feature importance based on statistical independence, while L1-based selection focuses on non-zero regression coefficients. They selected a total of 5 models; Naive Bayes, Decision Trees, SVMs, KNNs, and Random Forest. Overall, SVMs performed the best out of all the models with an accuracy of 72\%, closely followed by Naive Bayes with an accuracy of 70\%.

We also found a study that used ML approaches for authorship attribution in Arabic poetry \cite{arabic_poetry_attribution}. They established various characteristic of Arabic poetry such as \textit{Meter (Wazn)} and \textit{Rhyme (qafiya)}, and curated a corpus of Arabic poetry of 73 poets with 18646 Qasidah's for training that amount upto 1856436 words. They used SVMs and Naive Bayes for classification, using Chi-Square and Information Gain for feature selection. They came up with 6 features corresponding to character, word length, sentences length, first word length, meter, and rhyme. They achieved the highest accuracy of 98.86\% on SVMs when all features were used, while on average SVM had an accuracy of 87.4\%. They also implemented a Naive Bayes model which had the highest accuracy of 98.86\% as well with only two features, and an average accuracy of 90.68\%. 

Apart from the aforementioned literature, various other works were also consulted that were relevant to our research. One study we came across was the first to work on Pashto text classification using language processing techniques for single and multi-label analysis \cite{pashto_text_classification}. They constructed a dataset of Pashto documents including Sports, History, Health, Scientific, Cultural, Economic, Political, and Technology based - although poetry wasn't included. They used DistilBERT-base-multilingual-cased model, Multilayer Perceptron, SVMs, KNNs, Decision Trees, Gaussian Naive Bayes, Multinomial Naive Bayes, Random Forest, and Logistic Regression models. They got a 94\% accuracy using MLP classification and TFIDF feature extraction. DistilBERT - a multilingual model which was not pretrained on Pashto still was able to achieve 66.31\% accuracy, thus, the authors conclude that this still gave promising results, and that the model could be further improved by developing a tokenizer specifically tailored for Pashto.   